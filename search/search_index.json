{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Credit Team Docs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"CobolPrograms/IMSPrograms/","title":"IMS Programs","text":""},{"location":"CobolPrograms/IMSPrograms/#imm110","title":"IMM110","text":"<p>Description:  CREATES A REPORT OF P04, P05, P06, P07, W04, AND W06.                               </p> <p>THIS PROGRAM READS THROUGH THE IMS-AUTH DATABASE LOOKING FOR STATUS'S LISTED ABOVE AND CREATES A REPORT BASED ON WHAT     COMPANY THE PROGRAM IS CURRENTLY BEING RUN FOR.               </p> <p>Schedules:  *ALL     LTDALL </p> <p>Jobs:  *110J     LTD110J</p> <p>Notes:</p> <p>The way it is written, It will not create DR Leonards report in AMK110J because the ctl-company-cd = '04' so the sub-company-code is '0' and likewise in DRL110J the ctl-company-cd is 'A4' so the 04 records won't be on the report.</p> <p>9/28/22: DRL110-01 reports are now found in AMK 110-01 reports since the merging. Used by Tasha Smith</p> <p>B00 sql - Change Corporate Cd based on company being queried. Select * from IMS_AUTH where ((STATUS_CD = 'P' and STATUS_NO in ('04', '05', '06', '07')) or  (STATUS_CD = 'W' and STATUS_NO in ('04','06'))) and CORPORATE_CD = '4' order by CORPORATE_CD asc, SUB_COMP_CD asc, ADD_DATE asc;</p> <p>Parm Card usage:  CTL-TEST = Used only to change checkpoint from 5000 to 2 CTL-DISPLAY-ON = not used CTL-UPDATE-SW = not used CTL-COMPANY-CD = used to distinguish which records to write</p>"},{"location":"CobolPrograms/IMSPrograms/#imm430","title":"IMM430","text":"<p>Description:  PROCESS NEEDS AUTHORIZATION</p> <p>THIS PROGRAM SELECTS FROM THE AUTH DB ALL 'P06' UPDATING  THEM TO 'W06'. IT CALLS IMM108 TO REPORT.                </p> <p>Updates IMS-AUTH and IMS-CRED-QUEUE</p> <p>Schedules:  *ALL</p> <p>Jobs:  *430J</p> <p>Copybooks: </p> <p>COR742C STATE CD TABLE IMR108C 108 ATH RECORD COR946C Look up table IMR720L Lookup copybook</p> <p>Notes:</p> <p>The program pulls all records with corporate cd passed in parm card. It does not split DRL from AMK when writing to report. When reports were checked, both DRL and AMK records were found in the DRL430/431 and AMK430/431 reports. </p> <p>Process Normal: Select * from IMS_AUTH where CORPORATE_CD = '4' AND  ((STATUS_CD = 'P' AND STATUS_NO = '06') OR (STATUS_CD = 'P' AND STATUS_NO = '08'));</p> <p>E70-UPDATE Section: If Ath-Status = W06     Continue Else If Ath-Status = P06     Set status to W06     Update auth MOP run number to MPC MOP run number Else      Set status to W05</p> <p>If Build-control-rec upsi on and prev status is P06     If record found in IMS-CRED-QUEUE         Continue     If not found         Set zeros to              CCQ-MOVED-FROM-Q             CCQ-UPDATE-DATE              CCQ-UPDATE-TIME      F20,F30 --&gt; sets a bunch of fields for IMS-CRED-QUEUE      Updates or inserts to IMS-CRED-QUEUE</p>"},{"location":"CobolPrograms/IMSPrograms/#imm434","title":"IMM434","text":"<p>Jobs:  434J   UPSI(00000000)  Yes update, no requests 434J2  UPSI(01000000)  Requests to magnum 434J3  UPSI(00010000)  Request missing rpts 434J5  UPSI(10010000)  No update, request missing rpts</p> <p>Copybooks: </p> <p>Notes:</p> <pre><code>       UPSI-0 IS UPSI-UPDATE-DB-SW                                          \n          OFF IS UPSI-UPDATE-DB                                             \n       UPSI-1 IS UPSI-REQUESTS-TO-MAGNUM-SW                                 \n           ON IS UPSI-REQUESTS-TO-MAGNUM                                    \n       UPSI-2 IS UPSI-REQUEST-MISSING-APPR-SW                               \n           ON IS UPSI-REQUEST-MISSING-APPR                                  \n       UPSI-3 IS UPSI-REQUEST-MISSING-RPTS-SW                               \n           ON IS UPSI-REQUEST-MISSING-RPTS                                  \n       UPSI-4 IS UPSI-RUN-2-MAGNUM-FILES-SW                                 \n           ON IS UPSI-RUN-2-MAGNUM-FILES                                    \n       UPSI-5 IS UPSI-NO-MAGNUM-FILES-SW                                    \n           ON IS UPSI-NO-MAGNUM-FILES                                       \n       UPSI-6 IS UPSI-RUN-TEST-BATCH-SW                                     \n           ON IS UPSI-RUN-TEST-BATCH                                        \n       UPSI-7 IS UPSI-TEST-RUN-SW                                           \n           ON IS UPSI-TEST-RUN.\n</code></pre>"},{"location":"CobolPrograms/IMSPrograms/#imm444","title":"IMM444","text":"<p>Description:  THIS PROGRAM MERGES YTD TRACKING WITH THE  DAILY TRACKING FILE.                      </p> <p>Schedules:  *ALL</p> <p>Jobs:  *444J</p> <p>Copybooks:   CREDIT-TRACKING-RECORD     COPY IMR444C.                </p> <p>R490-MOP-TOTALS            COPY IMR490C9.               </p> <p>491 CALL AREA              COPY COR491C.                </p> <p>TRACKING-CODE-TABLE        *COPY COR723C.               COPY IMR723C.                </p> <p>Notes:</p> <p>Doesn't look like LTD/LSC specifically needs to be handled, as the program is file driven. </p> <p>For Figis and MKC, special coding is in there to print discount details to a PRINT-FILE-3 (SYS008) and PRINT-FILE-4 (SYS009), which are not used in DRL or AMK jobs. </p>"},{"location":"CobolPrograms/OnlinePrograms/","title":"Online Programs","text":""},{"location":"CobolPrograms/OnlinePrograms/#col335","title":"COL335","text":""},{"location":"CobolPrograms/OnlinePrograms/#description","title":"Description","text":"<p>This program is designed to accept bank card payments online.</p>"},{"location":"CobolPrograms/OnlinePrograms/#notes","title":"Notes","text":"<p>The program is designed to find orders that are Account Guard Eligible. It specifically looks for Account Guard Specific Conditions.</p>"},{"location":"CobolPrograms/OnlinePrograms/#sql-query","title":"SQL Query","text":"<p>The following SQL query is used to retrieve the necessary data:</p> <pre><code>select ath.batch_no, ath.order_seq, ord.CUSTOMER_NO, ath.DOWNPAY_OFFER_PCT, ord.BATCH_DATE, ord.ORDER_ACK_CD, scores.EXPERIAN_SCORE, scores.EQUIFAX_SCORE_04, scores.CRED_WORTHINESS, ath.TRK_RELEASE_CD, corsp.ACTION_CD\nfrom COP_AUTH ath\njoin COP_ORDER ord on (ath.BATCH_NO = ord.BATCH_NO and ath.ORDER_SEQ = ord.ORDER_SEQ)\njoin COP_CUSTOMER cus on (ord.CUSTOMER_NO = cus.CUSTOMER_NO)\njoin COP_CUS_SCORES scores on (ord.CUSTOMER_NO = scores.CUSTOMER_NO)\njoin COP_CORRSP corsp on (ath.BATCH_NO = corsp.BATCH_NO and ath.ORDER_SEQ = corsp.ORDER_SEQ)\nwhere ath.STATUS_CD='W' and ath.STATUS_NO = '05'\nand ath.COMPANY_CD in ('Q', 'G')\nand ath.DOWNPAY_OFFER_PCT &gt; '0.00' and ath.DOWNPAY_OFFER_PCT &lt;= '0.50'\nand ord.BATCH_DATE &gt; '20200615'    /********** make sure this is less than 20 days **********/\nand cus.STATE in ('AL','AK','AZ','AR','CT','DE','DC','FL','GA','GU','HI','ID','IL','KY','MI','MN','MS','MO','MT','NV','NH','NJ','NM','ND','OK','OR','RI','SD','TN','TX','VT','VI','WV')\nand SUBSTRING(lpad(cus.RANDOM_2, 16, 0), 12, 1) in (0 , 1, 2, 3, 4, 5, 6, 7)\nand ord.ORDER_ACK_CD not in ('X', 'C','D')\nand scores.EXPERIAN_SCORE &gt; '0' and scores.EQUIFAX_SCORE_04 &gt; '0' and scores.CRED_WORTHINESS &gt; '0'\n;\n</code></pre>"},{"location":"CobolPrograms/OnlinePrograms/#col337","title":"COL337","text":""},{"location":"CobolPrograms/OnlinePrograms/#description_1","title":"Description","text":"<p>This program is designed to accept ACH payments on Maggies. It is part of the COP - Catalog Order Processing system. The program accepts ACH bank information to allow a down payment on a credit held order (Maggie). For the initial start up of this application, only credit down payments under $200 are allowed. $200 and over are verified with the bank, and therefore excluded from this process. This program was copied from program COL335, which accepts bank card payments. ACH payments on A/R plans are also accepted. The process is similar to COL335. The ACH transaction is written to a VSAM file for batch processing via the A/R system. A 'BA' comment is written to the CORRSP file for batch process in the MOP system (COP471). COP471 applies the remittance to the order, and sets the appropriate tracking code on the AUTH record. This program does allow for same day deletes.</p>"},{"location":"CobolPrograms/OnlinePrograms/#testing","title":"Testing","text":"<p>This program is called from COL341. To get to this screen: GOTO CUS3, ENTER APPL ID CSR, ENTER AN AVAILABLE ORD# &amp; ADD. This will transfer to the COL341 screen, where you type in BA for the ACTION CD &amp; PF1. This will transfer to COL337  REPRICE COP122 COP-AUTH STATUS=W05 COP-ORDER DISC-OFFER-CD = DG</p>"},{"location":"CobolPrograms/OnlinePrograms/#schedules","title":"Schedules","text":"<p>n/a</p>"},{"location":"CobolPrograms/OnlinePrograms/#calling-programs","title":"Calling programs","text":"<ul> <li>COL219 - C/S ACTION NAVIGATION</li> <li>COL341 - CUSTOMER SERVICE CORRESPONDENCE ENTRY</li> <li>HAL169 - THANK YOU SCREEN WITH CREDIT MESSAGES</li> </ul>"},{"location":"CobolPrograms/OnlinePrograms/#copybooks","title":"Copybooks","text":"<p>n/a</p>"},{"location":"CobolPrograms/OnlinePrograms/#bde-elimination-notes","title":"BDE Elimination Notes","text":"<p>Overview of Changes: Have orders that do a down payment through COL337 send the order to RTC/Blaze to recalculate the BDE. As part of the RTC process, the CCQ (or BRC in COBOL) record will be completed, so the program doesn't have to do that anymore. We still need to call the COP484/COP485 path within the program to create the CAS audits because DM Services still uses it for their processes. COP472 makes a decision on the order and calculates the remaining order balance. We will need to save off the auth status, prev auth status, and tracking code fields to make sure the original values are reflected on the database before the Blaze call. If the Blaze call fails, we will set it to a P09 status so it can go through RTC at night through the Batch process. COP472 call updates the...</p> <p>[...]</p>"},{"location":"Programs/CatchLabelProcess/","title":"Catch Label Process","text":"<p>Order Velocity enhancement  - Customer exceeds order velocity count (F8 tracking release code)</p> <ul> <li>Assign a new tracking code - ??<ul> <li>Decline the order due to order velocity (already done)</li> <li>Add customer correspondence record? (do we do this already?)</li> <li>Find all other Approved orders that caused the velocity count to be exceeded (don\u2019t include Food)<ul> <li>Cancel orders not physically shipped  <ul> <li>Determine what\u2019s shipped on an order by looking at shipitem database</li> </ul> </li> <li>Assign a new tracking code of ??  - so what about partial shipments (some items on BO?) Which tracking code to apply?</li> <li>Credit the customer where appropriate  </li> <li>Add customer correspondence record(s)</li> </ul> </li> <li>Do not generate a letter</li> <li>Send those order(s) to HJ or team to track them down for those not physically shipped</li> </ul> </li> </ul> <p>Customer calls in with complaint about being charged for something they did not place an order for</p> <ul> <li>Cancel that order if appropriate</li> <li>Credit the customer for what they were charged</li> <li>Assign a new tracking code of ??</li> <li>Add customer correspondence record</li> <li>Do not generate a letter</li> <li>Find all other Approved orders \u2013 how would we know what was good &amp; bad?<ul> <li>Would they walk through all these with the customer on the phone?</li> </ul> </li> <li>Send those order(s) to HJ or team to track them down for those not physically shipped</li> </ul> <p>Catch Label or Stop order - Action Code LS, Program COP385 Kumar, Suresh - Aug 18, 2022</p> <p>How can you find out if shipping labels can be caught and order can be stopped?</p> <p>On CSR screen of an order action LS is entered. A job/program COP385J/COP385 runs every 15 minutes to look for LS action code, generates a report about labels if they can be stopped or not. That report helps customer service to make decision or take next action on orders usually on fraud orders.</p>"},{"location":"Programs/DailyCreditQueueCounts/","title":"Daily Credit Queue Counts","text":""},{"location":"Programs/DailyCreditQueueCounts/#request","title":"Request","text":"<p>Is there any way we could get a start of day credit que count?  There is an outsourcer that starts before we do.  The only way I know of to get our volumes is from the DWIP which is real time counts, so I cant get the start of day counts.</p> <p>Having to beginning day volumes better helps projections and planning.</p> <p>Thanks!</p> <p>Dawn White Sr. Manager, Credit Operations DM Services, Inc. (608) 324-6940 Please Consider the Environment Before Printing This E-mail </p>"},{"location":"Programs/DailyCreditQueueCounts/#requirements","title":"Requirements","text":"<ol> <li>Provide a report to DL-DMSSupervisoryStaff@sccompanies.com</li> <li>Don't include queues 35 or 00</li> <li>DWIP splits it by company, but don't need that for this report. Do need to...</li> <li>Split by Colony and Outside Clients</li> <li>Split by Unworked and CBR Back counts</li> <li>Report should run ideally before anyone gets in the system to work (5:00am).</li> <li>Runs at 4:30am, started on 11/18/20</li> </ol>"},{"location":"Programs/DailyCreditQueueCounts/#c-solution","title":"C# Solution","text":"<p>When triggered, it will read the database tables (COP_CRED_QUEUE and IMS_CRED_QEUE), format an email report, and then send the email to the respective group. The program lives in Credit Service Repo, under the creditservice project. The microservice has it's own controller it is driven off of and it's own endpoint. There is no cross between the prequal and credit queue counts process. </p>"},{"location":"Programs/DailyCreditQueueCounts/#change-notice","title":"Change Notice","text":"<p>RFC 25040 Work Order 25040 !7449</p>"},{"location":"Programs/DailyCreditQueueCounts/#program-structure","title":"Program Structure","text":"<ul> <li>Entry point into the program. You have to use the endpoint <code>web/creditqueuecounts</code> to be able call this microservice.</li> <li>The controller will format the email and call sub processes to gather data and send email.</li> <li>Credit_Queue_Counts_Flow_Diagram.vsdx</li> </ul>"},{"location":"Programs/DailyCreditQueueCounts/#queries","title":"Queries","text":"<ul> <li>COP_CRED_QUEUE</li> </ul> <pre><code>SELECT COP_CRED_QUEUE_QUEUE, PROCESS_STATUS, COUNT(*) from COP_CRED_QUEUE WHERE\n(PROCESS_STATUS = 'U' OR\nPROCESS_STATUS = 'B') AND\n(COP_CRED_QUEUE_QUEUE != '35' AND COP_CRED_QUEUE_QUEUE != '00') \ngroup by COP_CRED_QUEUE_QUEUE, PROCESS_STATUS \norder by COP_CRED_QUEUE_QUEUE, PROCESS_STATUS;\n</code></pre> <ul> <li>IMS_CRED_QUEUE</li> </ul> <pre><code>SELECT (CASE WHEN IMS_CRED_QUEUE_QUEUE &gt; '29' THEN (IMS_CRED_QUEUE_QUEUE - 8) ELSE IMS_CRED_QUEUE_QUEUE END) as 'QUEUE', PROCESS_STATUS as 'Type', COUNT(*) as 'COUNT' \nfrom IMS_CRED_QUEUE WHERE\n(PROCESS_STATUS = 'U' OR\nPROCESS_STATUS = 'B') AND\n(IMS_CRED_QUEUE_QUEUE != '17' AND IMS_CRED_QUEUE_QUEUE != '00') \ngroup by IMS_CRED_QUEUE_QUEUE, PROCESS_STATUS \norder by IMS_CRED_QUEUE_QUEUE, PROCESS_STATUS;\n</code></pre>"},{"location":"Programs/DailyCreditQueueCounts/#new-account","title":"New Account","text":"<p>In October of 2022 Credit Queue Counts was migrated to the new AWS account. It was made multi-regional, running on both us-east-1 and us-east-2. The tfstate file is kept in S3 bucket:</p> <ul> <li>Dev: S3: dev-terraform-infrastructure/projects/CreditQueueCounts</li> <li>UAT: S3: uat-terraform-infrastructure/projects/CreditQueueCounts </li> <li>PRD: S3: prd-terraform-infrastructure/projects/CreditQueueCounts  </li> </ul>"},{"location":"Programs/DailyCreditQueueCounts/#sns-notification","title":"SNS Notification","text":"<p>Go to AWS SNS and subscribe to the below topics to receive email reports.</p> <p>New Account Topics:</p> <ul> <li>dev-EmailService-CreditQueueCounts</li> <li>uat-EmailService-CreditQueueCounts</li> <li>prd-EmailService-CreditQueueCounts</li> </ul> <p>Old Account Topics:</p> <ul> <li>~~DevCreditQueueCounts~~</li> <li>~~UATCreditQueueCounts~~</li> <li>~~PRDCreditQueueCounts~~</li> </ul>"},{"location":"Programs/DailyCreditQueueCounts/#arns","title":"ARNs","text":"<p>New Account</p> <ul> <li> <p>arn:aws:sns:us-east-1:818019740876:dev-EmailService-CreditQueueCounts</p> </li> <li> <p>arn:aws:lambda:us-east-1:867146357054:function:uat-CreditQueueCounts</p> </li> <li> <p>arn:aws:lambda:us-east-2:867146357054:function:uat-CreditQueueCounts</p> </li> <li> <p>arn:aws:lambda:us-east-1:931583720194:function:prd-CreditQueueCounts</p> </li> <li>arn:aws:lambda:us-east-2:931583720194:function:prd-CreditQueueCounts</li> </ul> <p>Old Account ARNs</p> <ul> <li>~~arn:aws:sns:us-east-1:194344284237:DevCreditQueueCounts~~</li> <li>~~arn:aws:sns:us-east-1:194344284237:UATCreditQueueCounts~~</li> <li>~~arn:aws:sns:us-east-1:194344284237:PRDCreditQueueCounts~~</li> </ul> <p>SNS Notification Documentation</p>"},{"location":"Programs/DailyCreditQueueCounts/#api-gateway","title":"API Gateway","text":"<p>A new resource was added under dev-web api called creditqueuecounts</p> <p>New Account URL Endpoints</p> <ul> <li> <p>https://api.dev.sccompanies.com/dev-creditqueuecounts/creditqueuecounts</p> </li> <li> <p>https://gxw40afvka.execute-api.us-east-1.amazonaws.com/uat/creditqueuecounts</p> </li> <li>https://rkh0ttvvl6.execute-api.us-east-2.amazonaws.com/uat/creditqueuecounts/</li> <li> <p>https://api.uat.sccompanies.com/uat-creditqueuecounts/creditqueuecounts</p> </li> <li> <p>https://1sa7xh2ob4.execute-api.us-east-1.amazonaws.com/prd/creditqueuecounts</p> </li> <li>https://at2ef4zsv6.execute-api.us-east-2.amazonaws.com/prd/creditqueuecounts/</li> <li>https://api.prd.sccompanies.com/prd-creditqueuecounts/creditqueuecounts</li> </ul> <p>Old Account URL Endpoints</p> <ul> <li>~~https://apidev.sccompanies.com/web/creditqueuecounts~~</li> <li>~~https://apiuat.sccompanies.com/web/creditqueuecounts~~</li> <li>~~https://api.sccompanies.com/web/creditqueuecounts~~</li> </ul> <p>Each endpoint has a key associated with it that can be found in the gateway.  API Gateway Documentation </p>"},{"location":"Programs/DailyCreditQueueCounts/#automic-trigger-task","title":"Automic Trigger Task","text":"<p>An Automic task was created that will make a http request using curl. The task was added to @TIME schedule to be run at 4:30am daily (Mon-Sun).</p> <p>New Account Task Name</p> <ul> <li><code>JOBS.WIN.LAMBDA.CREDITQUEUECOUNTS</code> invokes command <code>curl -X POST -H \"x-api-key: \" &amp;APIENV#/&amp;RUNCLIENTLC#-creditqueuecounts/creditqueuecounts</code></li> </ul> <p>Old Account Task Name</p> <ul> <li><code>JOBS.WIN.LAMBDA.CREDITQUEUECOUNTS</code> invokes command <code>curl -X POST -H \"x-api-key: key-name\" &amp;APIENV#/web/creditqueuecounts</code></li> </ul> <p>The script is initiated on server AECPAS-WINAUT01.</p> <p>Job Name in @TIME  LAMBDA.credit.queue.counts</p> <p>Testing - Incorrectly name SNSCredQueueCounts environment name - For Task to have \"ERROR\" in the message - Use uat url on dev with dev x-api-key (this change is in Automic task) - For testing lambda connection</p>"},{"location":"Programs/DailyCreditQueueCounts/#error-handling","title":"Error Handling","text":"<p>If the email has the word 'ERROR' anywhere in it, the task will fail with a RC=1. Data Control will create a Numera Ticket and assign it to on-call (just like a batch failed job), then will call/contact the on-call person after 8:00am.</p> <p>If the database reads have errors and SNS worked, then users will get the email report that tells them to contact Credit Team, and Data Control will follow above procedure. </p> <p>If failed to send SNS messages but report was created, users will not receive email. Data Control will get RC=1 and follow above procedure. In this case, you can find the email in Cloudwatch logs for CreditService Lambda and manually send it to the users. </p> <p>If failed to trigger lambda, task will fail in Automic and Data Control will follow above procedure. </p> <p>On successful run, the task will receive RC=0 and users will receive email populated with report. </p>"},{"location":"Programs/IMSACHDownPayment/","title":"IMS ACH DownPayment","text":""},{"location":"Programs/IMSACHDownPayment/#api-overview","title":"API Overview","text":"<p>REPO: ImsAchDownpayment</p> <p>Swagger link: https://api.dev.sccompanies.com/dev-imsachdownpayment/swagger/index.html</p> <p>IMS ACH DP API URL: https://api.dev.sccompanies.com/dev-imsachdownpayment/api/ImsAchDownpayment</p> <p>S3 bucket: dev-imsachdownpayment-east1 dev-imsachdownpayment-east2 - nothing is written in it.</p> <p>Firehose Delivery streams: (delivery destination is always dev-imsachdownpayment-east1) dev-imsAchDpApiRequest dev-imsAchDpApiResponse dev-imsAchDpBlazeRequest dev-imsAchDpBlazeResponse</p> <p>Timestream database: dev-ImsAchDownpayment</p> <p>Timestream tables: dev-ImsAchDpApiRequest dev-ImsAchDpApiResponse dev-ImsAchDpBlazeRequest dev-ImsAchDpBlazeResponse</p> <p>Lambda: dev-ImsAchDownpayment</p> <p>SNS Topic: dev-IMSACHDownpaymentSNS</p> <p>Query for test data: </p> <pre><code>select concat(ath.BATCH_NO, ath.ORDER_SEQ, ath.ORDER_FILL) as ORDER_NO, ath.SUB_COMP_CD, ath.CORPORATE_CD, ord.CUSTOMER_NO, concat(ath.STATUS_CD, ath.STATUS_NO) as STATUS_CD, ath.DOWNPAY_OFFER_PCT, ath.DOWNPAY_AMT, ord.AMOUNT_DUE, ord.BATCH_DATE, ath.LTR_DISC_CD, ath.TRK_SUSPEND_CD, ath.TRK_RELEASE_CD\nfrom IMS_AUTH as ath\ninner join IMS_ORDER as ord on (ath.BATCH_NO = ord.BATCH_NO and ath.ORDER_SEQ = ord.ORDER_SEQ and ath.ORDER_FILL = ord.ORDER_FILL)\ninner join IMS_CUSTOMER as cus on (ord.CUSTOMER_NO = cus.CUSTOMER_NO)\nwhere ath.STATUS_CD = 'W' \nand ath.STATUS_NO = '05'\nand ath.DOWNPAY_OFFER_PCT &gt; '0.00' and ath.DOWNPAY_OFFER_PCT &lt; '0.50'\nand ath.ADD_DATE &gt; 20230901\nlimit 100;\n</code></pre>"},{"location":"Programs/IMSACHDownPayment/#down-payment-rules","title":"Down Payment Rules","text":"<p>IMS ACH Down Payment Rules:</p> <p>Customer with '0' credit limit and order in held status are not allowed to make ACH payment.</p> <pre><code>        select concat(ath.BATCH_NO, ath.ORDER_SEQ) as orderNbr, \n        cus.CUSTOMER_NO as customerNbr, \n        cus.CREDIT_LIMIT as customerLimit,\n        concat(ath.STATUS_CD, ath.STATUS_NO) as authStatus, \n        ath.TRK_RELEASE_CD as trkReleaseCd, \n        ath.DOWNPAY_AMT as downPaymentAmt, \n        ath.DOWNPAY_OFFER_PCT as downPaymentOfferPct,\n        ord.REMITTANCE as remittanceAmt,\n        ord.BALANCE_DUE as balanceDue\n        from IMS_AUTH as ath \n        inner join IMS_ORDER as ord on ath.BATCH_NO = ord.BATCH_NO and ath.ORDER_SEQ = ord.ORDER_SEQ\n        inner join IMS_CUSTOMER as cus on cus.CUSTOMER_NO = ord.CUSTOMER_NO\n        where (ath.STATUS_CD = 'P' or 'W') and cus.CREDIT_LIMIT = 0\n        order by ath.SEQUENCE_ID_KEY desc\n        limit 5;\n</code></pre> <p>If BLOCK_ACH_CD = 'Y' for customer on IR_CUSTOMER, ACH transaction not allowed.</p> <pre><code>    select CUSTOMER_NO, BLOCK_ACH_CD from IR_CUSTOMER where BLOCK_ACH_CD = 'Y' order by SEQUENCE_ID_KEY desc limit 1;\n\n    select CUSTOMER_NO, BLOCK_ACH_CD from IR_CUSTOMER where CUSTOMER_NO = '71022645608';\n</code></pre> <p>If customer not found in on IR_CUSTOMER, ACH transaction not allowed.</p> <p>If customer not found on IMS_CUS_SCORES table or if found customer, but Experian score and Equifax-score-10 is zero then ACH payment not allowed.</p> <pre><code>    select CUSTOMER_NO, COMPANY_CD, EXPERIAN_SCORE, EQUIFAX_SCORE_10 \n    from IMS_CUS_SCORES \n    where EXPERIAN_SCORE = 0 and EQUIFAX_SCORE_10 = 0 \n    order by SEQUENCE_ID_KEY desc\n    limit 10;\n\n    select CUSTOMER_NO, COMPANY_CD, EXPERIAN_SCORE, EQUIFAX_SCORE_10 \n    from IMS_CUS_SCORES \n    where CUSTOMER_NO = '2099669596' and COMPANY_CD = '02';\n</code></pre> <p>Only two ACH payment allowed to customer per company per season</p> <pre><code>    select CUSTOMER_NO, concat(SUB_COMP_CD, CORPORATE_CD) as companyCd, ACH_COUNT from IMS_CUSTOMER \n    where ACH_COUNT &gt; 2 \n    order by SEQUENCE_ID_KEY desc \n    limit 2;\n\n    select CUSTOMER_NO, ACH_COUNT from IMS_CUSTOMER\n    where CUSTOMER_NO = '2099669359' and SUB_COMP_CD = '0' and CORPORATE_CD = '2' ;\n</code></pre> <p>Can not accept ACH payment &gt; $200</p> <p>ACH not allowed if customer marked fraud in IR_CUSTOMER</p> <pre><code>    select CUSTOMER_NO, concat(SUB_COMP_CD, CORPORATE_CD) as companyCd, BAD_DEBT_CD from IMS_CUSTOMER \n    where BAD_DEBT_CD = 'F' \n    order by SEQUENCE_ID_KEY desc \n    limit 2;\n    select CUSTOMER_NO, concat(SUB_COMP_CD, CORPORATE_CD) as companyCd, BAD_DEBT_CD from IMS_CUSTOMER\n    where CUSTOMER_NO = '2099669359' and SUB_COMP_CD = '0' and CORPORATE_CD = '2' ;\n</code></pre> <p>ACH payment allowed only on held order, so if STATUS_CD != 'P' or 'W' or 'H', ACH payments not allowed. STATUS_CD in IMS_AUTH</p> <p>If down payment amount is less than minimum down payment required, then ACH payment is not allowed.</p> <pre><code>        COMPUTE WS-MINIMUM-PAYMENT\n                   = (ORD-BALANCE-DUE * WS-REQUESTED-PERCENT)\n</code></pre> <pre><code>Note - \nFrom IMS_AUTH, we can get DOWNPAY_OFFER_PCT, and DOWNPAY_AMT\nFrom IMS_ORDER , we can get BALANCE_DUE\n</code></pre> <p>Down payment amount cannot be more than order BALANCE_DUE. (We can get BALANCE_DUE from COP_ORDER) ACH is allowed with checking or saving account type *Check for ACH transaction in AR transaction file. If there is pending transaction, payments not allowed. We might need to review this scenario.</p> <p>Following are my are additional findings on ACH down payment rules for outside clients for specific message type on screen(highlighted):</p> <p>'SPEED PAY OPTION NOT AVAILABLE' results on following screens, when order in Process/pend or Waiting status and customer credit limit is zero.</p> <p>DSO screen (Program \u2013 IOL324) CSR screen (Program \u2013 IOL341) AUTH screen (Program \u2013 IOL347)</p> <p>Example:</p> <p>'ACH PAYMENT OPTION NOT AVAILABLE' results on CSR screen (Program -IOL341) under various conditions which are checked in program IMM239and they are as follows:     - If AR blocked ACH payment on AR_CUSTOMER table     - If customer not found on COP_CUS_SCORES table or if found customer, but Experian score and Equifax-score-10 is zero.     - There is another rule for corporate code \u20184\u2019, only two ACH payment allowed, I think this not relevant anymore.     - At some point for outside clients ACH payment over 200 wasn\u2019t accepted thru screen, they were asked to mail the check, not sure if that is still applicable (Found in program IOL337).</p>"},{"location":"Programs/IMSPegaTrainingRefresh/","title":"IMS Pega Training Data Refresh","text":""},{"location":"Programs/IMSPegaTrainingRefresh/#overview","title":"Overview","text":"<p>This project copies production data from Redshift over to the MySQL training environment. The purpose is to provide phone agents and operators with a consistent set of customer data for training purposes. The process uses several glue jobs and is automated using a couple automic schedules controlled by the AR team.</p> <p>The process consists of 5 Glue Jobs.</p>"},{"location":"Programs/IMSPegaTrainingRefresh/#glue-jobs-tables-and-resources","title":"Glue Jobs, Tables and Resources","text":"Glue Job MySqlTables Redshift env-CrdTrainingImsCusEmailExtract IMS_CUS_EMAIL oframe_prd_s.ims_cus_email env-dev-CrdTrainingImsMasonCrnExtract IMS_MASON_CRN oframe_cdc_prd_s.ims_mason_crn env-CrdTrainingMasonPreferencesExtract IMS_MASON_PREFERENCES oframe_prd_s.ims_mason_preferences env-CrdTrainingOrderAuthExtract IMS_AUTH, IMS_ORDER oframe_prd_s.ims_order, oframe_prd_s.ims_auth env-CrdTrainingImsCustomerExtract IMS_CUSTOMER oframe_prd_s.ims_customer <p>The 'Gold Copy' MySql tables are preceded with a GC_. The Following are the 'Gold Copy' Tables:</p> <pre><code>- GC_IMS_AUTH\n- GC_IMS_CUS_EMAIL\n- GC_IMS_CUSTOMER\n- GC_IMS_MASON_CRN\n- GC_IMS_MASON_PREFERENCES\n- GC_IMS_ORDER\n</code></pre>"},{"location":"Programs/IMSPegaTrainingRefresh/#automic-jobs","title":"Automic Jobs","text":"<p>This process is controlled by two Automic schedules:</p> <ol> <li> <p><code>JOBP.ARTRAIN_LOAD_IMS_GOLDCOPIES</code>: This schedule kicks off the 5 Credit glue jobs. These jobs pull new and updated production customer data from Redshift change-data-capture tables and loads it into the MySQL training \"gold copy\" tables.</p> </li> <li> <p><code>JOBP.ARTRAIN_IMS_REFRESH</code>: This schedule kicks-off a static load Glue jobs owned entirely by AR. These jobs copy data from the MySQL training \"gold copy\" tables into the main training tables. It runs much more frequently, multiple times per day, to reset the training data.</p> </li> </ol> <p>Note: The <code>JOBP.ARTRAIN_LOAD_IMS_GOLDCOPIES</code> schedule runs less often, only when new customers need to be added or existing customer data needs to be modified in the training environment.</p> <p>Note: The Glue Jobs owned by Credit are only run by the first Automic schedule, <code>JOBP.ARTRAIN_LOAD_IMS_GOLDCOPIES</code>. The second Automic Schedule only calls the static load Glue job which is entirely owned by AR.  </p> <p></p>"},{"location":"Programs/IMSPegaTrainingRefresh/#static-load-table","title":"Static Load Table","text":"<p>The AR team utilizes an AWS Glue job named <code>env-ARTrainingTableLoad</code>. This job is designed to transfer data from one MySQL table to another without making any modifications to the data. Essentially, it performs a one-to-one copy operation. </p> <p>The source (from) and destination (to) table names are provided as environmental variable arguments when running the Glue job. This job is primarily used to replicate the 'gold copy' tables (the original, unmodified tables) to the main tables that the Pega operators work with.</p> <p>It allows operators or agents to train, make changes and experiment with the data without affecting the 'gold copy' tables. Once the training session is complete, the environment can be quickly reset to its original state (using the 'gold copy' tables) in preparation for the next batch of trainees.</p> <p>Note: The management and control of this Glue job falls under the responsibility of the AR team, not the Credit team.</p>"},{"location":"Programs/IMSPegaTrainingRefresh/#adding-new-customers","title":"Adding New Customers","text":"<p>The AWS Glue jobs are designed to fetch a list of customer scenarios from a file stored in an S3 bucket named <code>env-training-scenarios-colonybrands-east1</code>. These customer scenarios are specifically chosen by the phone operators teams for training purposes. </p> <p>In addition to the customer data, a specific date is also passed into the Glue Job. This date is used to ensure that the Change Data Capture (CDC) data being retrieved corresponds to the same date.</p> <p>Any customer on the file will be added to the 'gold copy', but not removed. For instance, let's consider a scenario where the job is run twice. During the first run, the input file contains 10 customer numbers. These customers are added to the 'gold copy' tables. Now, for the second run, the input file has been updated and contains 8 different customers. After the second run, the 'gold copy' tables will contain data for a total of 18 customers (10 from the first run and 8 from the second run).</p> <p>The structure of the customer file is straightforward and might look something like this:</p> <p></p>"},{"location":"Programs/IMSPegaTrainingRefresh/#anonymizing-customer-data-in-glue-jobs","title":"Anonymizing Customer Data in Glue Jobs","text":"<p>Two of our AWS Glue jobs, <code>env-CrdTrainingImsCustomerExtract</code> and <code>env-CrdTrainingImsCusEmailExtract</code>, anonymize sensitive customer data. This is done to protect customer privacy and comply with data protection regulations.</p> <p>The anonymization of customer names in our system is based on the last four digits of the customer number. This process is important to understand as it directly impacts the resulting anonymized names.</p> <p>The first and last names are selected from a predefined list of 100 names, indexed from 0 to 99. The selection of these names is based on the last four digits of the customer number.</p> <p>Here's how it works:</p> <ol> <li> <p>The first name is selected based on the last two digits of the customer number. For example, if the last two digits are '92', the first name will be the 92nd name in the list (remember, the list is 0-indexed).</p> </li> <li> <p>The last name is selected based on the second to last two digits of the customer number.</p> </li> </ol> <p>It's important to note that the order of names in the list is crucial for this process to work correctly.</p> <p>List of first and last names used. </p> <pre><code>fnames = [\"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Eve\", \"Frank\", \"Grace\", \"Heidi\", \"Ivan\", \"Judy\", \"Karl\", \"Liam\", \"Mallory\", \"Nina\", \"Oscar\", \"Pat\", \"Quincy\", \"Rupert\", \"Sybil\", \"Trudy\", \"Ursula\", \"Victor\", \"Walter\", \"Xena\", \"Yanni\", \"Zelda\", \"Aaron\", \"Beth\", \"Caleb\", \"Diana\", \"Elijah\", \"Fiona\", \"Gabe\", \"Hannah\", \"Isaac\", \"Jasmine\", \"Kyle\", \"Laura\", \"Mason\", \"Natalie\", \"Oliver\", \"Paige\", \"Quinn\", \"Rachel\", \"Sam\", \"Tina\", \"Ulysses\", \"Violet\", \"William\", \"Xander\", \"Yara\", \"Zane\", \"Amy\", \"Blake\", \"Clara\", \"Derek\", \"Emma\", \"Felix\", \"Gina\", \"Henry\", \"Isla\", \"Jack\", \"Katie\", \"Luke\", \"Mia\", \"Noah\", \"Olivia\", \"Peter\", \"Quincy\", \"Rosa\", \"Steve\", \"Tara\", \"Uma\", \"Vince\", \"Wendy\", \"Xavier\", \"Yvonne\", \"Zack\", \"Alex\", \"Bella\", \"Carlos\", \"Dani\", \"Ethan\", \"Faye\", \"George\", \"Holly\", \"Ian\", \"Jill\", \"Ken\", \"Lily\", \"Max\", \"Nora\", \"Owen\", \"Polly\", \"Quentin\", \"Rita\", \"Sean\", \"Tess\", \"Una\", \"Vern\", \"Wanda\", \"Xerxes\", \"Yolanda\", \"Zeke\"]\n\nlnames = [\"Anderson\", \"Brown\", \"Carter\", \"Davis\", \"Evans\", \"Fisher\", \"Garcia\", \"Harris\", \"Iverson\", \"Johnson\", \"King\", \"Lewis\", \"Miller\", \"Nelson\", \"Owens\", \"Parker\", \"Quinn\", \"Robinson\", \"Smith\", \"Taylor\", \"Underwood\", \"Vaughn\", \"Williams\", \"Xavier\", \"Young\", \"Zimmerman\", \"Adams\", \"Baker\", \"Clark\", \"Drake\", \"Edwards\", \"Franklin\", \"Grant\", \"Hawkins\", \"Ingram\", \"Jackson\", \"Knight\", \"Lloyd\", \"Morgan\", \"Norton\", \"Olson\", \"Peterson\", \"Quinn\", \"Russell\", \"Stevens\", \"Turner\", \"Upton\", \"Vincent\", \"Watson\", \"Xanders\", \"York\", \"Zimmer\", \"Allison\", \"Boyd\", \"Chapman\", \"Dunn\", \"Ellis\", \"Fowler\", \"Griffin\", \"Hunt\", \"Irwin\", \"James\", \"Kelley\", \"Lawson\", \"Myers\", \"Norman\", \"O'Neil\", \"Patterson\", \"Quincy\", \"Reed\", \"Simpson\", \"Travis\", \"Underhill\", \"Vance\", \"Wallace\", \"Xiong\", \"Yates\", \"Zane\", \"Arnold\", \"Blake\", \"Caldwell\", \"Dixon\", \"Emerson\", \"Freeman\", \"Glover\", \"Henderson\", \"Irving\", \"Jennings\", \"Kennedy\", \"Lambert\", \"Murphy\", \"Nolan\", \"Osborne\", \"Payne\", \"Quigley\", \"Riley\", \"Sanders\", \"Thompson\", \"Upshaw\", \"Vega\", \"Webb\", \"Xu\", \"Young\", \"Zavala\"]\n</code></pre>"},{"location":"Programs/OrderHook/","title":"OrderHook","text":""},{"location":"Programs/OrderHook/#overview","title":"Overview","text":"<p>The OrderHook program is invoked with an order number, which it uses to query the MySQL COP_AUTH table. If the order has an 'F8' track release code, it signifies potential multi-order fraud. The program then checks for any other orders from the same customer within the last day. If such orders are found, an email alert is sent for potential cancellation. These orders' details are logged in the <code>env-OrderHook</code> DynamoDB table on the old corp AWS account.</p>"},{"location":"Programs/OrderHook/#important-queries","title":"Important Queries","text":"<pre><code>var query = $@\"select ATH.TRK_RELEASE_CD, ORD.CUSTOMER_NO, ORD.BATCH_DATE \nfrom COP_AUTH ATH\njoin COP_ORDER ORD\non ATH.BATCH_NO = ORD.BATCH_NO and ATH.ORDER_SEQ = ORD.ORDER_SEQ\nwhere ATH.BATCH_NO = '{ordNo.Substring(0, 6)}' and ATH.ORDER_SEQ = '{ordNo.Substring(6, 2)}'\";\n</code></pre> <pre><code>var query = $@\"select concat(ATH.BATCH_NO, ATH.ORDER_SEQ), ORD.TOTAL_FOR_POST as ordno from COP_AUTH ATH\njoin COP_ORDER ORD on ATH.BATCH_NO = ORD.BATCH_NO and ATH.ORDER_SEQ = ORD.ORDER_SEQ\nwhere ORD.CUSTOMER_NO = '{customerNo}' and (ORD.BATCH_DATE = '{batchDate}' or ORD.BATCH_DATE = '{dayBefore}')\nand (ORD.COP_ORDER_STATUS = 'N' or ORD.COP_ORDER_STATUS = 'F') and ATH.TRK_RELEASE_CD &lt;&gt; ' F8' and ATH.STATUS_CD = 'A'\";\n</code></pre>"},{"location":"Programs/OrderHook/#program-location","title":"Program location","text":"<p>The program is old and currently sits in the product teams repo. The project is under a solution called <code>CallCenterOrder</code> and the actual C# project is called <code>siftOrderCall</code>. The lambda it run on is in the old AWS corp accout and runs on a lambda named  <code>CallCenterOrderPlace</code>.</p>"},{"location":"Programs/Prequal/","title":"Prequal","text":""},{"location":"Programs/Prequal/#new-endpoints","title":"New Endpoints","text":""},{"location":"Programs/Prequal/#mason","title":"Mason","text":"<ul> <li>https://api.dev.sccompanies.com/dev-mason-prequal/mason-prequal</li> <li>https://api.uat.sccompanies.com/uat-mason-prequal/mason-prequal</li> <li>https://api.prd.sccompanies.com/prd-mason-prequal/mason-prequal</li> </ul>"},{"location":"Programs/Prequal/#colony","title":"Colony","text":"<ul> <li>https://api.dev.sccompanies.com/dev-colony-prequal/prequal/</li> <li>https://api.uat.sccompanies.com/uat-colony-prequal/prequal</li> <li>https://api.prd.sccompanies.com/prd-colony-prequal/prequal</li> </ul>"},{"location":"Programs/Prequal/#response-codes","title":"Response Codes","text":"<ul> <li>00 - Good prequal offer for brand new person (updates promo tables)</li> <li>10 - Denied prequal from Blaze rules</li> <li>11 - No Hit from Bureau Indicators</li> <li>12 - Denied from Bureau Indicators</li> <li>*13 - Denied Bad Debt</li> <li>*14 - Denied Slow Pay</li> <li>20 - Service deactivated</li> <li>30 - Existing prequal offer<ul> <li>IMS - found on IMS_PROMO</li> <li>Colony - valid offer with prequal flag = \"Y\"</li> </ul> </li> <li>*31 - Existing Buyer with valid limit for the company</li> <li>*32 - Existing Buyer, new prequal offer</li> <li>*33 - Existing Promo with valid limit for the company</li> <li>*34 - Existing Promo, new prequal offer</li> <li>*40 - Denied Vermont</li> <li>90 - Internal Error</li> <li>91 - Invalid Company Code Error</li> <li>92 - Database Error (reading or connection issues)</li> <li>93 - Bureau Error (executing or parsing returned info)</li> <li>94 - Blaze Error (executing or parsing returned info)</li> <li>95 - Insert Database Error (writing to a table)</li> <li>96 - Spectrum Error (anything from calling to storing returned values)</li> <li>*97 - Matchcode Error</li> <li>*Colony Specific Code</li> </ul>"},{"location":"Programs/Prequal/#expiration-date-logic","title":"Expiration Date Logic","text":""},{"location":"Programs/Prequal/#ims","title":"IMS","text":"<ul> <li>If they get a prequal limit November through April, we will expire those July 31th.</li> <li>If they get a prequal limit May through October, we will expire those January 31st.</li> </ul>"},{"location":"Programs/Prequal/#colony_1","title":"Colony","text":"<ul> <li>If they get a prequal limit November through April, we will expire those June 30th.</li> <li>If they get a prequal limit May through October, we will expire those December 31st.</li> </ul>"},{"location":"Programs/Prequal/#prequal-audit-redshift","title":"Prequal Audit RedShift","text":"<p>Detailed list - PreQual Redshift table Audits</p>"},{"location":"Programs/Prequal/#production-table","title":"Production Table","text":"<p>crd_s.prequal</p>"},{"location":"Programs/Prequal/#dev-table","title":"Dev Table","text":"<p>crd_dev_s.prequal Credit team \u2013 you\u2019ll need to expand the EXTERNAL TABLE folder and then you will see the prequal table</p>"},{"location":"Programs/Prequal/#tips-and-tricks","title":"Tips and Tricks:","text":"<ol> <li>Add year, month, day when querying to make it execute faster.</li> <li>Colony Key = Cookie Id. This may not be unique</li> <li>Mason Key = Finder Number</li> </ol>"},{"location":"Programs/Prequal/#samplequeries","title":"SampleQueries","text":"<pre><code>select count(pres_offeramount1) \nfrom crd_s.prequal \nwhere pres_offeramount1 &gt; 0 and responsecode = '34';\n\nselect pres_requestid, customerno, preq_firstname, preq_lastname, preq_address1, company, date, responsecode, \npres_offeramount1, pres_offercode1, pres_offermessage1, pres_offerexpirationdate1  \nfrom crd_s.prequal \nwhere company = 'colony' and pres_clientid = 'Q' and date &gt; '01/22/2020'\nlimit 25;\n\nSelect month, day, key, responsecode, blzq_riskscore, blzs_riskrank, blzs_rules_idfield, blzs_rules_valfield, burs_nohitindicator, burs_scoretext, burs_reasoncode1, burs_reasoncode2, burs_reasoncode3, burs_reasoncode4, burs_retcode  \nfrom crd_s.prequal \nwhere preq_clientid = 'G2' and year = '2020' order by key;\n\nSelect * from crd_s.prequal where year = '2020' and month = '08' and day = '01' limit 1;\n\nSelect key, companyid, responsecode, pres_score, blzq_findernbr,blzs_riskrank, burs_irregularreportdeceasedindicator, burs_consumerstatementindicator, burs_abnormalreportindicator, burs_addressmismatchindicator, burs_nohitindicator, pres_offeramount1, year, month, day \nfrom crd_s.prequal \nwhere company = 'colony' and year = '2020' and month = '05' \norder by year, month, day asc;\n\nSelect * from crd_dev_s.prequal where KEY = '10207122550615862045249' AND\nyear = '2020' and month = '04' and day = '06'  order by date;\n\nselect companyid, pres_responsecode, count(*)\nfrom crd_s.prequal \nwhere year = '2023' and month = '01' and day in ('04', '05') \ngroup by companyid, pres_responsecode;\n\n</code></pre>"},{"location":"Programs/Prequal/#ad-codes","title":"Ad Codes","text":"<p>Advertisement codes (Ad codes or Adv codes) are unique codes that identify a marketing season, year and catalog. These codes are used for sales reporting. The LST team owns and maintains the table, along with Ami Barker and Kevin Geib's team. For prequal, Ami Barker assigns the ad code using a screen on for production. LST team (Deb or Heidi) assign the same codes in development. In the past, with permission giving from LST team, I have copied the new production codes into dev to test. </p> <p>Ad codes are stored on the MRK_AD_CODE table in MySQL. There is no easy key that we can use to find the prequal codes, so this was the fastest way I could grab only the records I needed. </p> <p>Select * from MRK_AD_CODE where DESC_1 like '%PREQUAL%'; 02/15/22 - Code enhancement to look up Adv Code based on Company_CD + Season_Year + %PREQUAL%, which removed the need to hardcode each ad code. Season End Year End Prequal update every 6 months  When business requests a new season cut over, CRD team will need to update the SeasonYear field in the environment variable of the lambda. Ex: S22 or F22. Note: The format of DESC_1 in the database should follow this format example MW S22 SI PREQUAL Not doing so will result in default 9999[suffix] adv codes and warning emails to CRD team.</p> <p>Typically, the ad codes are active in December/January and June/July timeframes</p> <p>Ad code load new, current goes to prev Already a valid offer &gt; write to hold file and reapply after end of season exp.      Try to reapply any limits that were in the hold file  Plan code - yes update for both      Buyers - happens when ad code is added.</p> <p>Read MRK ad code table and pull plan codes from that too when applying. </p> <p>Installment then open plan &gt; July 1st, only limits are updated, not ad code or plan codes.      Equal limit or lower, then update limit</p> <p>Amy Barker - promo updates Credit - customer updates</p> <p>Anyone who received prior plan O or plan I will be forced into the same plan in this mailing.      Promo - plan will not change on the customers.     If promo has higher expiration date, then only will the record be added/updated.      Always loading offer ad code and plan, but may not upload limit. </p> <p>Ad code and plan code don't expire. Only limits will expire. </p>"},{"location":"Programs/Prequal/#prequal-audit-locations","title":"Prequal Audit Locations","text":"<p>Production Table: crd_s.prequal</p> <p>Dev Table: crd_dev_s.prequal</p> <p>Cloudwatch Log groups API Gateway Logs These are good to look at to see if user is hitting the api gateway. Either they have a permissions issue, api key issue, or possibly not hitting it at all. </p> <p>API-Gateway-Execution-Logs_lalzi7idnl/prd (colony east-1) API-Gateway-Execution-Logs_eehsrcydhi/prd (colony east-2)</p> <p>API-Gateway-Execution-Logs_e9es7y2415/prd (mason east-1) API-Gateway-Execution-Logs_fppbw90w77/prd (mason east-2)</p>"},{"location":"Programs/Prequal/#zipcode-test-cases","title":"ZipCode Test Cases","text":"<p>The test cases have been expanded to cover more scenarios and now rely on the last two digits of the zip code. Below are more details around the changes:</p> <ul> <li>22 test cases</li> <li>Use the last 2 digits of the zip code to trigger each test scenario</li> <li>Each test case is associated with a rank. See the \"Rank-Score Mapping\" tab to know what you can expect for test cases between 6 and 19.<ul> <li>Example: zip ending in ***10 = rank 10, which maps to score 890</li> </ul> </li> <li>I have a note on some of the test cases indicating there will be different results due to the volume split test in our rules engine. I gave an example of each scenario between the different companies for that row. </li> </ul> <p>See the \"Volume Split Test\" tab for expectations.</p>"},{"location":"Programs/creditLimitDecisioning/","title":"CreditLimitDecisioning","text":""},{"location":"Programs/creditLimitDecisioning/#overview","title":"Overview","text":"<p>The credit limit decisioning process determines if a customer's credit limit can be increased. The entire process is controlled with an Automic schedule called CREDIT_BLAZE_LIMITS. It starts by kicking off a series of data warehouse jobs. These jobs compile relevant customer data into a Redshift table called tempwork.creditlimitrequest.</p> <p>Next, an AWS Glue job named {env}-CreditLimitDecisioningGlueJob copies the customer records from the Redshift table into a MySQL table called CRD_CREDIT_LIMIT_BLAZE_DATA.</p> <p>The Lambda function process retrieves the customer data from the MySQL table and feeds it into the Blaze rules engine, which contains the logic around credit limits. The output of the Blaze rules engine is saved into the CRD_CREDIT_LIMIT_BLAZE_RESPONSES Table.</p> <p>Finally, these blaze response records are picked up by COP381US. This COBOL program creates an input file for the normal COP381 program, which will pick up the file and update the customers credit limits. </p> <p></p>"},{"location":"Programs/creditLimitDecisioning/#lambda-function-process-overview","title":"Lambda Function Process Overview","text":"dev uat prd Lambda 1 dev-CreditLimitDecisioningPushRecords uat-CreditLimitDecisioningPushRecords prd-CreditLimitDecisioningPushRecords Lambda 2 dev-CreditLimitDecisioning uat-CreditLimitDecisioning prd-CreditLimitDecisioning <p>This process relies on two AWS Lambda functions and an SQS queue to process customer records through Blaze Rules Engine.</p> <p>The Automic schedule will spin up nine {env}-CreditLimitDecisioningPushRecords Lambda executions. These lambdas, running concurrently, pull data from CRD_CREDIT_LIMIT_BLAZE_DATA table. Each Lambda is assigned a number from 1 to 9. When invoked, the Lambdas will only query records where the LAMBDA_NUMBER column value matches their assigned number. This strategy allows the large volume of customer records to be divided into manageable batches across each lambda execution. This parallelizes the processing. The {env}-CreditLimitDecisioningPushRecords Lambda also checks the  PROCESSED_STATUS column. If this column is empty or null, it means the record has not yet been processed. If it contains 'Y', the record has already been processed. The loader lambda will only retrieve records not yet processed. After each {env}-CreditLimitDecisioningPushRecords Lambda queried it's data subset, it loads that data into an AWS sqs.The data, called messages, are loaded into an SQS queue called {env}-CreditLimitDecisioning.</p> <p>Important: The loader uses a fire-and-forget system. It does not check the response after sending a request to load messages into the queue. Messages can fail to load. If this occurs, the queue should be purged and the records reloaded.</p> <p>The queue holds these messages until they are processed. When the trigger is enabled on the {env}-CreditLimitDecisioning Lambda function, it will retrieve messages from the queue. These messages include SequenceIdkey numbers. The program takes that unique key and queries the customer data from the CRD_CREDIT_LIMIT_BLAZE_DATA MySQL table. It then formats that data into an XML SOAP request. Using SOAP, a request is made to the Blaze Server. The program receives an XML response from Blaze, which it reformats and saves into the CRD_CREDIT_LIMIT_BLAZE_RESPONSES MySQL table.</p> <p>If a message is processed successfully, it is deleted from the queue. If a message fails processing, it goes back to the queue to be reprocessed. If a message continues failing after multiple attempts, it is sent to the {env}-CreditLimitDecisioningDeadLetter queue. In the deadletter queue, failed messages sit until purged. Developers can pull records from the deadletter queue through the AWS console to investigate why the messages failed.</p> <p>The {env}-CreditLimitDecisioning Lambda function that processes messages from the SQS queue can scale up dynamically based on demand. The maximum concurrency (max number of simultaneous Lambda executions) is configured in Terraform. </p> <p>This allows the processing capacity to increase exponentially. When the Lambda is triggered, it will launch up to the max concurrent executions if needed, each of which retrieves messages from the queue in batches and processes them. For example if the Max Concurrency is set to 75, a max of 75 lambda executions will work in parallel to process messages in the queue. </p> <p>The batching window, batch size, and other parameters that control the throughput and parallelism of the Lambdas are also configurable in the Terraform resource block. For example:</p> <pre><code>resource \"aws_lambda_event_source_mapping\" \"sqstrigger-east1\" {\n  provider          = aws.east1\n  event_source_arn  = aws_sqs_queue.main-queue-east1.arn\n  function_name     = module.lambda_alias-east1-main.lambda_alias_arn\n  function_response_types = [\"ReportBatchItemFailures\"]\n  maximum_batching_window_in_seconds = 300\n  batch_size = 1000\n  scaling_config {\n    maximum_concurrency = 75\n  }\n  depends_on = [\n    aws_sqs_queue.main-queue-east1,\n    module.lambda_alias-east1-main\n  ]\n}\n</code></pre> <p>The following terraform resource block configures the number of times that a message can be retried and the visibility timeout for each message. </p> <pre><code>resource \"aws_sqs_queue\" \"main-queue-east1\" {\n    provider = aws.east1\n    visibility_timeout_seconds = 900\n    name = \"${local.env}-${var.lambdaname_main}\"\n    redrive_policy = jsonencode({\n    deadLetterTargetArn = aws_sqs_queue.dead-letter-east1.arn\n    maxReceiveCount     = 3\n  })\n}\n</code></pre> <p>Both the request and response, along with the raw XML request and response, are saved in Redshift.</p>"},{"location":"Programs/creditLimitDecisioning/#audits-and-mysql-tables","title":"Audits and MySql Tables","text":"<p>Redshift Tables</p> Table Type Description tempwork.creditlimitrequest Redshift data is initially loaded here creditlimitdecisioning_dev_s.creditlimitdecisioningblazerequesta Redshift blaze request audit creditlimitdecisioning_dev_s.creditlimitdecisioningxmlblazerequest Redshift raw xml request audit creditlimitdecisioning_dev_s.creditlimitdecisioningblazeresponse Redshift blaze response audit creditlimitdecisioning_dev_s.creditlimitdecisioningxmlblazeresponse Redshift raw xml response audit CRD_CREDIT_LIMIT_BLAZE_DATA MySql copied from tempwork.creditlimitrequest CRD_CREDIT_LIMIT_BLAZE_RESPONSES MySql stores response from blaze <p>Redshift Audits</p> <p>Audit data is captured by the program and sent to an S3 bucket named {env}-creditlimitdecisioning-colonybrands-east1 using Kinesis Firehose. The data is in JSON format. Kinesis automatically merges individual JSON audits into larger single JSON files.</p> <p>After the JSON files are saved in S3, a Glue crawler called {env}_CreditLimitDecisioning runs and picks up any new files. These files are then added to a Glue catalog database table and loaded into Redshift.</p> <p>Note: The crawler must run to get new audits into Redshift. The crawler runs at the end of the automatic schedule to pick up new audit files, but can also be triggered manually.</p>"},{"location":"Programs/creditLimitDecisioning/#sql-queries-table-creation","title":"SQL Queries - Table Creation","text":"<p>The following SQS queries will create both the needed MySQL tables to make the program work.</p>"},{"location":"Programs/creditLimitDecisioning/#crd_credit_limit_blaze_data-table","title":"CRD_CREDIT_LIMIT_BLAZE_DATA Table","text":"<pre><code>CREATE TABLE `CRD_CREDIT_LIMIT_BLAZE_DATA` (\n  `SEQUENCE_ID_KEY` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `ACCOUNT_NUMBER` varchar(9) NOT NULL,\n  `COMPANY_CODE` varchar(2) NOT NULL,\n  `PROCESSED_STATUS` varchar(10) DEFAULT NULL,\n  `TEST_CODE` varchar(5) DEFAULT NULL,\n  `LAMBDA_NUMBER` int(9) NOT NULL,\n  `ADD_TIMESTAMP` datetime DEFAULT CURRENT_TIMESTAMP,\n  `UPDATE_TIMESTAMP` timestamp NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n  `LIFE_PAYMENT_AMT_CMP` decimal(9,2) DEFAULT '0.00',\n  `LIFE_NSF_CHECK_AMT_CMP` decimal(9,2) DEFAULT '0.00',\n  `MAX_BALANCE_CMP` decimal(9,2) DEFAULT '0.00',\n  `LAST_CMP_PAYMENT_DATE` varchar(10) DEFAULT 'T',\n  `MAX_TRANSACTION_DATE_CMP` varchar(10) DEFAULT 'T',\n  `PAST_DUE_DAYS_AFTER_CMP` int(9) DEFAULT '0',\n  `BALANCE_AMT_CMP` decimal(9,2) DEFAULT '0.00',\n  `MAX_PD_STATUS_L3M_CMP` varchar(1) DEFAULT 'T',\n  `MAX_PD_STATUS_L4M_CMP` varchar(1) DEFAULT 'T',\n  `LAST_CLOSING_DATE_CMP` varchar(10) DEFAULT 'T',\n  `LIFE_TOTAL_PAID_AMT_CMP` decimal(9,2) DEFAULT '0.00',\n  `LIFE_PD_COUNT_CMP` int(9) DEFAULT '0',\n  `LIFE_CLOSING_COUNT_CMP` int(9) DEFAULT '0',\n  `WORST_PAST_DUE_STATUS_CMP` varchar(1) DEFAULT 'T',\n  `LIFE_NSF_COUNT_CMP` int(9) DEFAULT '0',\n  `LIFE_SALES_CMP` decimal(9,2) DEFAULT '0.00',\n  `LIFE_ORDERS_CMP` int(9) DEFAULT '0',\n  `SALES_LAST_6M_CMP` decimal(9,2) DEFAULT '0.00',\n  `APPROVED_ORDERS_LAST_6M_CMP` int(9) DEFAULT '0',\n  `HELD_DEMAND_LAST_6M_CMP` decimal(9,2) DEFAULT '0.00',\n  `HELD_ORDERS_LAST_6M_CMP` int(9) DEFAULT '0',\n  `DP_AMOUNT_LAST_6M_CMP` decimal(9,2) DEFAULT '0.00',\n  `DP_ORDERS_LAST_6M_CMP` int(9) DEFAULT '0',\n  `BAD_DEBT_PLACEMENT_DATE_CMP` varchar(10) DEFAULT 'T',\n  `BD_LAST_12M_FLAG` varchar(5) DEFAULT 'T',\n  `CREDIT_LIMIT_CMP` decimal(9,2) DEFAULT '0.00',\n  `CREDIT_LIMIT_SET_DATE` varchar(10) DEFAULT 'T',\n  `CREDIT_LIMIT_EXPIRATION_DATE` varchar(10) DEFAULT 'T',\n  `CREDIT_LIMIT_LOGIC_CODE` varchar(3) DEFAULT 'T',\n  `XCMP_BD_MAILED_CODE` varchar(1) DEFAULT 'T',\n  `INCR_LIMIT_EXPIRATION_DATE` varchar(10) DEFAULT 'T',\n  `INCR_CREDIT_LIMIT` decimal(9,2) DEFAULT '0.00',\n  `NO_CHANGE_TO_LIMIT_FLAG` varchar(5) DEFAULT 'T',\n  `CLOSED_ACCT_FLAG` varchar(5) DEFAULT 'T',\n  `BAD_DEBT_CODE` varchar(1) DEFAULT 'T',\n  `RANDOM_NBR_3` varchar(16) DEFAULT 'T',\n  `AFFILIATE_SHARING_FLAG` varchar(5) DEFAULT 'T',\n  `INCOME_AMT` decimal(9,2) DEFAULT '0.00',\n  `INCOME_DATE` varchar(10) DEFAULT 'T',\n  `CBR_SCORE_4` decimal(3,2) DEFAULT '0.00',\n  `CBR_EXPIRATION_DATE` varchar(10) DEFAULT 'T',\n  `EXPERIAN_RISK_SCORE` int(3) DEFAULT '0',\n  `EXPERIAN_EXPIRATION_DATE` varchar(10) DEFAULT 'T',\n  `EQUIFAX_04_RISK_SCORE` int(3) DEFAULT '0',\n  `EQUIFAX_04_EXPIRATION_DATE` varchar(10) DEFAULT 'T',\n  `EQUIFAX_10_RISK_SCORE` int(3) DEFAULT '0',\n  `EQUIFAX_10_EXPIRATION_DATE` varchar(10) DEFAULT 'T',\n  `CASH_BANK_CARD_SALES_CMP` decimal(9,2) DEFAULT '0.00',\n  `LAST_FOE_DATE_CMP` varchar(10) DEFAULT 'T',\n  `FIRST_FOE_DATE_CMP` varchar(10) DEFAULT 'T',\n  `TRANS_UNION_RISK_SCORE` int(3) DEFAULT '0',\n  `TRANS_UNION_EXPIRATION_DATE` varchar(10) DEFAULT NULL,\n  `CBR_ANNUAL_INCOME` decimal(9,2) DEFAULT '0.00',\n  `CBR_MONTHLY_EXPENSE` decimal(9,2) DEFAULT '0.00',\n  `CREDIT_WORTHINESS_SCORE` int(3) DEFAULT '0',\n  `RISK_SCORE_CODE` varchar(1) DEFAULT 'T',\n  `WORST_PD_NON_BD_COMPANIES` int(6) DEFAULT '0',\n  `BAD_DEBT_COMPANY_COUNT` int(9) DEFAULT '0',\n  `MAX_CORP_FOE_DATE` varchar(10) DEFAULT 'T',\n  `CORP_LIMIT_COUNT` int(9) DEFAULT '0',\n  `DAYS_SINCE_LAST_INCREASE` int(9) DEFAULT '0',\n  `DAYS_SINCE_LAST_FOE_CORP_ORDER` int(9) DEFAULT '0',\n  `DAYS_SINCE_FIRST_FOE_ORDER` int(9) DEFAULT '0',\n  `LAST_CORP_FOE_DATE` varchar(10) DEFAULT 'T',\n  `FIRST_CORP_FOE_DATE` varchar(10) DEFAULT 'T',\n  `LIFE_PAYMENT_AMT_CORP` decimal(9,2) DEFAULT '0.00',\n  `LIFE_NSF_CHECK_AMT_CORP` decimal(9,2) DEFAULT '0.00',\n  `MAX_BALANCE_CORP` decimal(9,2) DEFAULT '0.00',\n  `LAST_REMIT_DATE_CORP` varchar(10) DEFAULT 'T',\n  `MAX_TRANSACTION_DATE_CORP` varchar(10) DEFAULT 'T',\n  `PAST_DUE_DAYS_AFTER_CORP` int(9) DEFAULT '0',\n  `BALANCE_AMT_CORP` decimal(9,2) DEFAULT '0.00',\n  `MAX_PD_STATUS_L3M_CORP` varchar(1) DEFAULT 'T',\n  `MAX_PD_STATUS_L4M_CORP` varchar(1) DEFAULT 'T',\n  `LAST_CLOSING_DATE_CORP` varchar(10) DEFAULT 'T',\n  `LIFE_PD_COUNT_CORP` int(9) DEFAULT '0',\n  `LIFE_CLOSING_COUNT_CORP` int(9) DEFAULT '0',\n  `WORST_PAST_DUE_STATUS_CORP` varchar(5) DEFAULT 'T',\n  `LIFE_NSF_COUNT_CORP` int(9) DEFAULT '0',\n  `LIFE_SALES_CORP` decimal(9,2) DEFAULT '0.00',\n  `LIFE_ORDERS_CORP` int(9) DEFAULT '0',\n  `SALES_LAST_6M_CORP` decimal(9,2) DEFAULT '0.00',\n  `APPROVED_ORDERS_LAST_6M_CORP` int(9) DEFAULT '0',\n  `HELD_DEMAND_LAST_6M_CORP` decimal(9,2) DEFAULT '0.00',\n  `HELD_ORDERS_LAST_6M_CORP` int(9) DEFAULT '0',\n  `DP_AMOUNT_LAST_6M_CORP` decimal(9,2) DEFAULT '0.00',\n  `DP_ORDERS_LAST_6M_CORP` int(9) DEFAULT '0',\n  `CASH_BANK_CARD_SALES_CORP` decimal(9,2) DEFAULT '0.00',\n  `BD_LAST_12M_CORP_FLAG` varchar(5) DEFAULT 'T',\n  `CBR_EXP_MINUS_6M` varchar(29) DEFAULT 'T',\n  `EQUIFAX_04_EXP_MINUS_6M` varchar(29) DEFAULT 'T',\n  `EQUIFAX_10_EXP_MINUS_6M` varchar(29) DEFAULT 'T',\n  `EXPERIAN_EXP_MINUS_6M` varchar(29) DEFAULT 'T',\n  `TRANS_UNION_EXP_MINUS_6M` varchar(29) DEFAULT 'T',\n  `TODAY_MINUS_6M` varchar(32) DEFAULT 'T',\n  `CREDIT_LIMIT_TYPE` varchar(6) DEFAULT 'T',\n  `CMP_AR_PAYMENT_TOTAL` decimal(38,2) DEFAULT '0.00',\n  `CORP_AR_PAYMENT_TOTAL` decimal(38,1) DEFAULT '0.0',\n  `RUN_DATE` int(18) DEFAULT '0',\n  PRIMARY KEY (`SEQUENCE_ID_KEY`),\n  UNIQUE KEY `CUST_COMP_KEY` (`ACCOUNT_NUMBER`,`COMPANY_CODE`),\n  KEY `IDX_ADD_TIMESTAMP` (`ADD_TIMESTAMP`),\n  KEY `IDX_TYPE_OF_CREDIT_LIMIT` (`CREDIT_LIMIT_TYPE`),\n  KEY `IDX_PROCESSED_STATUS` (`PROCESSED_STATUS`),\n  KEY `IDX_COMPANY_AND_STATUS` (`COMPANY_CODE`, `PROCESSED_STATUS`)\n) ENGINE=InnoDB AUTO_INCREMENT=10427569 DEFAULT CHARSET=latin1;\n</code></pre>"},{"location":"Programs/creditLimitDecisioning/#crd_credit_limit_blaze_response-table","title":"CRD_CREDIT_LIMIT_BLAZE_RESPONSE Table","text":"<pre><code>CREATE TABLE `CRD_CREDIT_LIMIT_BLAZE_RESPONSES` (\n  `SEQUENCE_ID_KEY` bigint(20) unsigned NOT NULL AUTO_INCREMENT,\n  `FK_SEQUENCE_ID_KEY` bigint(20) NOT NULL,\n  `CREDIT_LIMIT_TYPE` char(1) DEFAULT ' ',\n  `UPDATE_FLAG` char(5) DEFAULT '     ',\n  `ACCOUNT_NUMBER` char(9) DEFAULT '         ',\n  `COMPANY_CODE` char(2) DEFAULT '7 ',\n  `ADD_TIMESTAMP` datetime DEFAULT CURRENT_TIMESTAMP,\n  `UPDATE_TIMESTAMP` timestamp NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n  `OUTSIDE_RISK_SCORE` int(3) DEFAULT '0',\n  `OUTSIDE_SCORE_RANK` int(3) DEFAULT '0',\n  `OUTSIDE_SCORE_SOURCE` char(5) DEFAULT '     ',\n  `OUTSIDE_SCORE_EXPIRATION_DATE` char(10) DEFAULT '          ',\n  `RISK_PROBABILITY_SCORE` int(3) DEFAULT '0',\n  `RESIDUAL_INCOME_AMT` decimal(9,0) DEFAULT '0',\n  `PERMANENT_LIMIT_FLAG` char(5) DEFAULT '     ',\n  `INCREASE_QUALIFY_FLAG` char(5) DEFAULT NULL,\n  `CREDIT_LIMIT_CAP` decimal(9,2) DEFAULT '0.00',\n  `STEP_AMOUNT` decimal(9,0) DEFAULT '0',\n  `HIGH_BALANCE_FACTOR` int(9) unsigned zerofill DEFAULT '000000000',\n  `STARTING_CREDIT_LIMIT` decimal(9,0) DEFAULT '0',\n  `NEW_CREDIT_LIMIT` decimal(9,0) DEFAULT '0',\n  `NEW_LIMIT_EXPIRATION_DATE` char(10) DEFAULT '          ',\n  `NEW_LIMIT_SET_DATE` char(10) DEFAULT '          ',\n  `NEW_LOGIC_CODE` decimal(3,0) DEFAULT '0',\n  `CREDIT_LIMIT_RESULT` char(3) DEFAULT '   ',\n  `STEP_CREDIT_LIMIT` decimal(9,0) DEFAULT '0',\n  `COMPANY_TYPE` char(10) DEFAULT '          ',\n  `NEW_MIN_PAYMENT_AMT` decimal(9,2) DEFAULT '0.00',\n  `NEW_XCMP_CREDIT_LIMIT` decimal(9,0) DEFAULT '0',\n  `XCMP_LOGIC_CODE` decimal(3,0) DEFAULT '0',\n  `WITHIN_CMP_LOGIC_CODE` decimal(3,0) DEFAULT '0',\n  `PAYMENT_AFTER_SET_DATE_FLAG` char(5) DEFAULT '     ',\n  `CUSTOMER_TYPE` char(10) DEFAULT '          ',\n  `CROSS_COMPANY_TO_SET_FLAG` char(5) DEFAULT ' ',\n  `WITHIN_COMPANY_TO_SET_FLAG` char(5) DEFAULT ' ',\n  `LIMIT_STEP_FLAG` char(5) DEFAULT '     ',\n  `XCMP_EXPIRATION_DATE` char(10) DEFAULT '          ',\n  `EXPIRE_LIMIT_FLAG` char(5) DEFAULT '     ',\n  `INCREASE_ELIGIBLE_FLAG` char(5) DEFAULT '     ',\n  `XCMP_QUALIFY_FLAG` char(45) DEFAULT NULL,\n  `ORIG_INCREASE_QUALIFY_FLAG` char(45) DEFAULT NULL,\n  `HIGH_BALANCE_LIMIT` decimal(9,2) DEFAULT NULL,\n  `MAX_HIGH_BALANCE_FACTOR` decimal(9,2) DEFAULT NULL,\n  `STEPPED_LIMIT` decimal(9,2) DEFAULT NULL,\n  `RELEASE_VERSION` varchar(45) DEFAULT NULL,\n  PRIMARY KEY (`SEQUENCE_ID_KEY`),\n  UNIQUE KEY `CUST_COMP_KEY` (`ACCOUNT_NUMBER`,`COMPANY_CODE`),\n  KEY `IDX_ADD_TIMESTAMP` (`ADD_TIMESTAMP`),\n  KEY `IDX_UPDATE_FLAG` (`UPDATE_FLAG`),\n  KEY `IDX_TYPE_OF_CREDIT_LIMIT` (`CREDIT_LIMIT_TYPE`)\n) ENGINE=InnoDB AUTO_INCREMENT=4828953 DEFAULT CHARSET=latin1;\n</code></pre>"},{"location":"Programs/creditLimitDecisioning/#restart-procedure","title":"Restart Procedure","text":"<p>Job restarts can be entirely managed in the Automic schedule JOBP.CREDIT_BLAZE_LIMITS.</p> <p></p> <p>1: Glue Job Failure</p> <p>If the glue job (step GLUEJOB.DATALOAD) fails, this step can be restarted. Any remnants from a partially completed glue job run, or any previous run, will be cleaned up. It will truncate both the CRD_CREDIT_LIMIT_BLAZE_RESPONSES and CRD_CREDIT_LIMIT_BLAZE_DATA MySql table before it begins copying over data from the tempwork.creditlimitrequest Redshift table. </p> <p>2: Any Other Failure</p> <p>If the Glue job (GLUEJOB.DATALOAD) was successful, and some step after it failed, the schedule can then be restarted at step BASH.PURGE.AND.DISABLE. This step will purge the queue of any old messages and then proceed like normal.  </p> <p>The database table CRD_CREDIT_LIMIT_BLAZE_DATA in the PROCESSED_STATUS column keeps track of which records have been processed by the {env}-CreditLimitDecisioning lambda and which ones have not. If there is ever an instance where some records looks like they were missed, restarting at step BASH.PURGE.AND.DISABLE will pick up the missed records without touching those that have already been processed.</p> <p>There's a rare chance that the {env}-CreditLimitDecisioning lambda will encounter duplicate customer records and the C# code should automatically handle these correctly. </p>"},{"location":"Programs/FraudService/FraudService/","title":"Overview","text":""},{"location":"Programs/FraudService/FraudServiceMSI/","title":"FraudServiceMSI","text":""},{"location":"Programs/FraudService/FraudServiceMSI/#description","title":"Description:","text":"<p>This API was created to be utilized by the online screens CSR (COL341) and Authorization work (COL458) to invoke the Fraud Service Lambda via a MySQL call. The batch program to process letters and audits (COP435) also calls the service. The service simply takes in a Fraud Service request and passes it along to the Fraud Service Lambda via the API Gateway for processing. This service was set up in such a way because the Fraud Service Lambda call only be triggered from the API Gateway. AWS Lambdas are limited to one event and a setting up a generic event to handle multiple events is not easily done.</p>"},{"location":"Programs/FraudService/FraudServiceMSI/#code-and-repo","title":"Code and Repo:","text":"<p>Lives under the Fraud Repo.</p>"},{"location":"Programs/FraudService/FraudServiceMSI/#sample-request","title":"Sample Request:","text":"<pre><code>SELECT \nlambda_sync( 'arn:aws:lambda:us-east-1:194344284237:function:dev-FraudServiceMSI', \nconcat('\"{\\'requestType\\':\\'', 'fraudreject', '\\',\\'calling_pgm\\':\\'', 'COL341', '\\',\\'orderNumber\\':\\'', '87649839' ,'\\'}\"'));\n</code></pre>"},{"location":"Programs/FraudService/FraudServiceMSI/#sample-response","title":"Sample Response:","text":"<p>Response returns to the database as a JSON object in a BLOB</p> <pre><code>{\n    \"status\": \"93\",\n    \"statusMessage\": \"Error in Kount: No valid Kount data found on database.\",\n    \"outOfWalletFlag\": \"N\"\n}\n</code></pre>"},{"location":"Programs/FraudService/FraudServiceMSI/#cobol-code","title":"Cobol Code:","text":""},{"location":"Programs/FraudService/GoodOrderCheck/","title":"GoodOrderCheck","text":""},{"location":"Programs/FraudService/GoodOrderCheck/#overview","title":"Overview","text":"<p>GoodOrderCheckGetCustomers will query AR_DETAIL table and get a list of customers that made payments the day before it runs. Runs daily at 10AM and kicked off by Automic.  GoodOrderCheck Lambda will take in a customer number and company code from an SQS. Will query COP_ORDER and return all the orders that the customer has ever made for that company.</p> <p>ProcessMessage controller will then query COP_FRAUDSERVICE and check the DECISION_ON_SIFT column. If this column has not already been set with a value (hasn't already been marked good), it'll call FraudService API with 'LegitOrder' call type. FraudSerivce will then update the order as being good. </p>"},{"location":"Programs/FraudService/GoodOrderCheck/#diagram","title":"Diagram","text":""},{"location":"Programs/FraudService/GoodOrderCheck/#troubleshooting","title":"TroubleShooting","text":""},{"location":"Programs/FraudService/Kount/","title":"Kount","text":""},{"location":"Programs/FraudService/Sift/","title":"Sift","text":""},{"location":"Programs/FraudService/Sift/#sift-status","title":"Sift Status","text":"<p>Official Sift Health Dashboard</p>"},{"location":"Programs/FraudService/Sift/#sift-nuget-package","title":"Sift Nuget Package","text":"<p>We use the official Sift nuget package to interact with Sift api.</p> <p>Github documentation for .net version of nuget package.</p> <p>Official documentation for the sift api.</p> <p>Official Nuget Package:</p> <p></p>"}]}